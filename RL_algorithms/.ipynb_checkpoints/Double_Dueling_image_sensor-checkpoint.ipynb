{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Based Self Driving Car Control (Image + Sensor)\n",
    "\n",
    "## Double DQN with Dueling Architecture\n",
    "\n",
    "This notebook is DRL code for the project 'DRL based Self Driving Car Control' <br>\n",
    "This version uses both **Image data from camera** and **Sensor data from LIDAR** as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment path\n",
    "\n",
    "Be sure to set `env_name` to the name of the Unity environment file you want to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"../environment/jeju_camp\" # Name of the Unity environment to launch\n",
    "train_mode = True # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the environment\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the observation and state spaces\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, _states_ refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, _observations_ refer to a set of relevant pixel-wise visuals for an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Sensor data (LIDAR): \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "Num_obs = len(env_info.visual_observations)\n",
    "\n",
    "print(\"Image data (Front Camera): \\n{}\")\n",
    "if Num_obs > 1:\n",
    "    f, axarr = plt.subplots(1, Num_obs, figsize=(20,10))\n",
    "    for i, observation in enumerate(env_info.visual_observations):\n",
    "        if observation.shape[3] == 3:\n",
    "            axarr[i].imshow(observation[0,:,:,:])\n",
    "            axarr[i].axis('off')\n",
    "        else:\n",
    "            axarr[i].imshow(observation[0,:,:,0])\n",
    "            axarr[i].axis('off')\n",
    "else:\n",
    "    f, axarr = plt.subplots(1, Num_obs)\n",
    "    for i, observation in enumerate(env_info.visual_observations):\n",
    "        if observation.shape[3] == 3:\n",
    "            axarr.imshow(observation[0,:,:,:])\n",
    "            axarr.axis('off')\n",
    "        else:\n",
    "            axarr.imshow(observation[0,:,:,0])\n",
    "            axarr.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "algorithm = 'DDuelingDQN'\n",
    "Num_action = brain.vector_action_space_size\n",
    "\n",
    "# parameter for DQN\n",
    "Num_replay_memory = 100000\n",
    "Num_start_training = 50000\n",
    "Num_training = 1000000\n",
    "Num_update = 10000\n",
    "Num_batch = 32\n",
    "Num_test = 100000\n",
    "Num_skipFrame = 4\n",
    "Num_stackFrame = 4\n",
    "Num_colorChannel = 1\n",
    "\n",
    "Epsilon = 1.0\n",
    "Final_epsilon = 0.1\n",
    "Gamma = 0.99\n",
    "Learning_rate = 0.00025\n",
    "\n",
    "# Parameter for LSTM\n",
    "Num_dataSize = 366\n",
    "Num_cellState = 512\n",
    "\n",
    "# Parameters for network\n",
    "img_size = 80\n",
    "sensor_size = 360\n",
    "\n",
    "first_conv   = [8,8,Num_colorChannel * Num_stackFrame * Num_obs,32]\n",
    "second_conv  = [4,4,32,64]\n",
    "third_conv   = [3,3,64,64]\n",
    "first_dense  = [10*10*64 + Num_cellState, 512]\n",
    "second_dense_state  = [first_dense[1], 1]\n",
    "second_dense_action = [first_dense[1], Num_action]\n",
    "\n",
    "# Path of the network model\n",
    "load_path = '../saved_networks/2018-05-04_10_40_DDuelingDQN_both/model.ckpt'\n",
    "\n",
    "# Parameters for session\n",
    "Num_plot_episode = 5\n",
    "Num_step_save = 50000\n",
    "\n",
    "GPU_fraction = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(xavier_initializer(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "\treturn tf.Variable(xavier_initializer(shape))\n",
    "\n",
    "# Xavier Weights initializer\n",
    "def xavier_initializer(shape):\n",
    "\tdim_sum = np.sum(shape)\n",
    "\tif len(shape) == 1:\n",
    "\t\tdim_sum += 1\n",
    "\tbound = np.sqrt(2.0 / dim_sum)\n",
    "\treturn tf.random_uniform(shape, minval=-bound, maxval=bound)\n",
    "\n",
    "# Convolution function\n",
    "def conv2d(x,w, stride):\n",
    "\treturn tf.nn.conv2d(x,w,strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "# Assign network variables to target network\n",
    "def assign_network_to_target():\n",
    "\t# Get trainable variables\n",
    "\ttrainable_variables = tf.trainable_variables()\n",
    "\t# network lstm variables\n",
    "\ttrainable_variables_network = [var for var in trainable_variables if var.name.startswith('network')]\n",
    "\n",
    "\t# target lstm variables\n",
    "\ttrainable_variables_target = [var for var in trainable_variables if var.name.startswith('target')]\n",
    "\n",
    "    # assign network variables to target network\n",
    "\tfor i in range(len(trainable_variables_network)):\n",
    "\t\tsess.run(tf.assign(trainable_variables_target[i], trainable_variables_network[i]))\n",
    "\n",
    "# Code for tensorboard\n",
    "def setup_summary():\n",
    "    episode_speed      = tf.Variable(0.)\n",
    "    episode_overtake   = tf.Variable(0.)\n",
    "    episode_lanechange = tf.Variable(0.)\n",
    "\n",
    "    tf.summary.scalar('Average_Speed/' + str(Num_plot_episode) + 'episodes', episode_speed)\n",
    "    tf.summary.scalar('Average_overtake/' + str(Num_plot_episode) + 'episodes', episode_overtake)\n",
    "    tf.summary.scalar('Average_lanechange/' + str(Num_plot_episode) + 'episodes', episode_lanechange)\n",
    "\n",
    "    summary_vars = [episode_speed, episode_overtake, episode_lanechange]\n",
    "    summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "    update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Input\n",
    "x_image = tf.placeholder(tf.float32, shape = [None, img_size, img_size, Num_colorChannel * Num_stackFrame * Num_obs])\n",
    "x_normalize = (x_image - (255.0/2)) / (255.0/2)\n",
    "\n",
    "x_sensor = tf.placeholder(tf.float32, shape = [None, Num_stackFrame, Num_dataSize])\n",
    "x_unstack = tf.unstack(x_sensor, axis = 1)\n",
    "\n",
    "with tf.variable_scope('network'):\n",
    "    # Convolution variables\n",
    "    w_conv1 = weight_variable(first_conv)\n",
    "    b_conv1 = bias_variable([first_conv[3]])\n",
    "\n",
    "    w_conv2 = weight_variable(second_conv)\n",
    "    b_conv2 = bias_variable([second_conv[3]])\n",
    "\n",
    "    w_conv3 = weight_variable(third_conv)\n",
    "    b_conv3 = bias_variable([third_conv[3]])\n",
    "\n",
    "    # Densely connect layer variables\n",
    "    w_fc1_1 = weight_variable(first_dense)\n",
    "    b_fc1_1 = bias_variable([first_dense[1]])\n",
    "\n",
    "    w_fc1_2 = weight_variable(first_dense)\n",
    "    b_fc1_2 = bias_variable([first_dense[1]])\n",
    "\n",
    "    w_fc2_1 = weight_variable(second_dense_state)\n",
    "    b_fc2_1 = bias_variable([second_dense_state[1]])\n",
    "\n",
    "    w_fc2_2 = weight_variable(second_dense_action)\n",
    "    b_fc2_2 = bias_variable([second_dense_action[1]])\n",
    "    \n",
    "    # LSTM cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units = Num_cellState)            \n",
    "    rnn_out, rnn_state = tf.nn.static_rnn(inputs = x_unstack, cell = cell, dtype = tf.float32)\n",
    "    \n",
    "# Network\n",
    "h_conv1 = tf.nn.relu(conv2d(x_normalize, w_conv1, 4) + b_conv1)\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, w_conv2, 2) + b_conv2)\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, w_conv3, 1) + b_conv3)\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_conv3, [-1, 10 * 10 * 64])\n",
    "rnn_out = rnn_out[-1]\n",
    "h_concat = tf.concat([h_pool3_flat, rnn_out], axis = 1)\n",
    "\n",
    "h_fc1_state  = tf.nn.relu(tf.matmul(h_concat, w_fc1_1)+b_fc1_1)\n",
    "h_fc1_action = tf.nn.relu(tf.matmul(h_concat, w_fc1_2)+b_fc1_2)\n",
    "\n",
    "h_fc2_state  = tf.matmul(h_fc1_state,  w_fc2_1)+b_fc2_1\n",
    "h_fc2_action = tf.matmul(h_fc1_action, w_fc2_2)+b_fc2_2\n",
    "\n",
    "h_fc2_advantage = tf.subtract(h_fc2_action, tf.reduce_mean(h_fc2_action))\n",
    "\n",
    "output = tf.add(h_fc2_state, h_fc2_advantage)\n",
    "\n",
    "with tf.variable_scope('target'):\n",
    "    # Convolution variables target\n",
    "    w_conv1_target = weight_variable(first_conv)\n",
    "    b_conv1_target = bias_variable([first_conv[3]])\n",
    "\n",
    "    w_conv2_target = weight_variable(second_conv)\n",
    "    b_conv2_target = bias_variable([second_conv[3]])\n",
    "\n",
    "    w_conv3_target = weight_variable(third_conv)\n",
    "    b_conv3_target = bias_variable([third_conv[3]])\n",
    "\n",
    "    # Densely connect layer variables target\n",
    "    w_fc1_1_target = weight_variable(first_dense)\n",
    "    b_fc1_1_target = bias_variable([first_dense[1]])\n",
    "\n",
    "    w_fc1_2_target = weight_variable(first_dense)\n",
    "    b_fc1_2_target = bias_variable([first_dense[1]])\n",
    "\n",
    "    w_fc2_1_target = weight_variable(second_dense_state)\n",
    "    b_fc2_1_target = bias_variable([second_dense_state[1]])\n",
    "\n",
    "    w_fc2_2_target = weight_variable(second_dense_action)\n",
    "    b_fc2_2_target = bias_variable([second_dense_action[1]])\n",
    "\n",
    "    # LSTM cell\n",
    "    cell_target = tf.contrib.rnn.BasicLSTMCell(num_units = Num_cellState)            \n",
    "    rnn_out_target, rnn_state_target = tf.nn.static_rnn(inputs = x_unstack, cell = cell_target, dtype = tf.float32)\n",
    "    \n",
    "# Target Network\n",
    "h_conv1_target = tf.nn.relu(conv2d(x_normalize, w_conv1_target, 4) + b_conv1_target)\n",
    "h_conv2_target = tf.nn.relu(conv2d(h_conv1_target, w_conv2_target, 2) + b_conv2_target)\n",
    "h_conv3_target = tf.nn.relu(conv2d(h_conv2_target, w_conv3_target, 1) + b_conv3_target)\n",
    "\n",
    "h_pool3_flat_target = tf.reshape(h_conv3_target, [-1, 10 * 10 * 64])\n",
    "rnn_out_target = rnn_out_target[-1]\n",
    "h_concat_target = tf.concat([h_pool3_flat_target, rnn_out_target], axis = 1)\n",
    "\n",
    "h_fc1_state_target  = tf.nn.relu(tf.matmul(h_concat_target, w_fc1_1_target)+b_fc1_1_target)\n",
    "h_fc1_action_target = tf.nn.relu(tf.matmul(h_concat_target, w_fc1_2_target)+b_fc1_2_target)\n",
    "\n",
    "h_fc2_state_target  = tf.matmul(h_fc1_state_target,  w_fc2_1_target)+b_fc2_1_target\n",
    "h_fc2_action_target = tf.matmul(h_fc1_action_target, w_fc2_2_target)+b_fc2_2_target\n",
    "\n",
    "h_fc2_advantage_target = tf.subtract(h_fc2_action_target, tf.reduce_mean(h_fc2_action_target))\n",
    "\n",
    "output_target = tf.add(h_fc2_state_target, h_fc2_advantage_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and Train\n",
    "action_target = tf.placeholder(tf.float32, shape = [None, Num_action])\n",
    "y_target = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "y_prediction = tf.reduce_sum(tf.multiply(output, action_target), reduction_indices = 1)\n",
    "Loss = tf.reduce_mean(tf.square(y_prediction - y_target))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = Learning_rate, epsilon = 1e-02).minimize(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize variables\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = GPU_fraction\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training or Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file if the saved file exists\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# check_save = 1\n",
    "check_save = input('Inference? / Training?(1=Inference/2=Training): ')\n",
    "\n",
    "if check_save == '1':\n",
    "    # Directly start inference\n",
    "    Num_start_training = 0\n",
    "    Num_training = 0\n",
    "    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, load_path)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "# date - hour - minute of training time\n",
    "date_time = str(datetime.date.today()) + '_' + str(datetime.datetime.now().hour) + '_' + str(datetime.datetime.now().minute)\n",
    "\n",
    "# Make folder for save data\n",
    "os.makedirs('../saved_networks/' + date_time + '_' + algorithm + '_both')\n",
    "\n",
    "# Summary for tensorboard\n",
    "summary_placeholders, update_ops, summary_op = setup_summary()\n",
    "summary_writer = tf.summary.FileWriter('../saved_networks/' + date_time + '_' + algorithm + '_both', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize input \n",
    "def input_initialization(env_info):\n",
    "    # Observation\n",
    "    observation_stack_obs = np.zeros([img_size, img_size, Num_colorChannel * Num_obs])\n",
    "    \n",
    "    for i in range(Num_obs):\n",
    "        observation = 255 * env_info.visual_observations[i]\n",
    "        observation = np.uint8(observation)\n",
    "        observation = np.reshape(observation, (observation.shape[1], observation.shape[2], 3))\n",
    "        observation = cv2.resize(observation, (img_size, img_size))\n",
    "\n",
    "        if Num_colorChannel == 1:\n",
    "            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "            observation = np.reshape(observation, (img_size, img_size))\n",
    "\n",
    "        if Num_colorChannel == 3:\n",
    "            observation_stack_obs[:,:, Num_colorChannel * i: Num_colorChannel * (i+1)] = observation\n",
    "        else:\n",
    "            observation_stack_obs[:,:, i] = observation\n",
    "\n",
    "    observation_set = []\n",
    "\n",
    "    # State\n",
    "    state = env_info.vector_observations[0][:-7]\n",
    "    state_set = []\n",
    "        \n",
    "    for i in range(Num_skipFrame * Num_stackFrame):\n",
    "        observation_set.append(observation_stack_obs)\n",
    "        state_set.append(state)\n",
    "    \n",
    "    # Stack the frame according to the number of skipping and stacking frames using observation set\n",
    "    observation_stack = np.zeros((img_size, img_size, Num_colorChannel * Num_stackFrame * Num_obs))\n",
    "    state_stack = np.zeros((Num_stackFrame, Num_dataSize))\n",
    "    \n",
    "    for stack_frame in range(Num_stackFrame):\n",
    "        observation_stack[:,:,Num_obs * stack_frame: Num_obs * (stack_frame+1)] = observation_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "        state_stack[(Num_stackFrame - 1) - stack_frame, :] = state_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "    \n",
    "    observation_stack = np.uint8(observation_stack)\n",
    "    state_stack = np.uint8(state_stack)\n",
    "    \n",
    "    return observation_stack, observation_set, state_stack, state_set\n",
    "\n",
    "# Resize input information \n",
    "def resize_input(env_info, observation_set, state_set):\n",
    "    # Stack observation according to the number of observations\n",
    "    observation_stack_obs = np.zeros([img_size, img_size, Num_colorChannel * Num_obs])\n",
    "\n",
    "    for i in range(Num_obs):\n",
    "        observation = 255 * env_info.visual_observations[i]\n",
    "        observation = np.uint8(observation)\n",
    "        observation = np.reshape(observation, (observation.shape[1], observation.shape[2], 3))\n",
    "        observation = cv2.resize(observation, (img_size, img_size))\n",
    "        \n",
    "        if Num_colorChannel == 1:\n",
    "            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "            observation = np.reshape(observation, (img_size, img_size))\n",
    "\n",
    "        if Num_colorChannel == 3:\n",
    "            observation_stack_obs[:,:, Num_colorChannel * i: Num_colorChannel * (i+1)] = observation\n",
    "        else:\n",
    "            observation_stack_obs[:,:,i] = observation\n",
    "    \n",
    "    # Add observations to the observation_set\n",
    "    observation_set.append(observation_stack_obs)\n",
    "    \n",
    "    # State \n",
    "    state = env_info.vector_observations[0][:-7]\n",
    "\n",
    "    # Add state to the state_set\n",
    "    state_set.append(state)\n",
    "    \n",
    "    # Stack the frame according to the number of skipping and stacking frames using observation set\n",
    "    observation_stack = np.zeros((img_size, img_size, Num_colorChannel * Num_stackFrame * Num_obs))\n",
    "    state_stack = np.zeros((Num_stackFrame, Num_dataSize))\n",
    "\n",
    "    for stack_frame in range(Num_stackFrame):\n",
    "        observation_stack[:,:,Num_obs * stack_frame: Num_obs * (stack_frame+1)] = observation_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "        state_stack[(Num_stackFrame - 1) - stack_frame, :] = state_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "\n",
    "    del observation_set[0]\n",
    "    del state_set[0]\n",
    "    \n",
    "    observation_stack = np.uint8(observation_stack)\n",
    "    state_stack = np.uint8(state_stack)\n",
    "        \n",
    "    return observation_stack, observation_set, state_stack, state_set\n",
    "\n",
    "# Get progress according to the number of steps\n",
    "def get_progress(step, Epsilon):\n",
    "    if step <= Num_start_training:\n",
    "        # Observation\n",
    "        progress = 'Observing'\n",
    "        train_mode = True\n",
    "        Epsilon = 1\n",
    "    elif step <= Num_start_training + Num_training:\n",
    "        # Training\n",
    "        progress = 'Training'\n",
    "        train_mode = True\n",
    "        \n",
    "        # Decrease the epsilon value\n",
    "        if Epsilon > Final_epsilon:\n",
    "            Epsilon -= 1.0/Num_training\n",
    "    elif step < Num_start_training + Num_training + Num_test:\n",
    "        # Testing\n",
    "        progress = 'Testing'\n",
    "        train_mode = False\n",
    "        Epsilon = 0\n",
    "    else:\n",
    "        # Finished\n",
    "        progress = 'Finished'\n",
    "        train_mode = False\n",
    "        Epsilon = 0\n",
    "        \n",
    "    return progress, train_mode, Epsilon \n",
    "\n",
    "# Select action according to the progress of training\n",
    "def select_action(progress, sess, observation_stack, state_stack, Epsilon):\n",
    "    if progress == \"Observing\":\n",
    "        # Random action \n",
    "        Q_value = 0\n",
    "        action = np.zeros([Num_action])\n",
    "        action[random.randint(0, Num_action - 1)] = 1.0\n",
    "    elif progress == \"Training\":\n",
    "        # if random value(0-1) is smaller than Epsilon, action is random. \n",
    "        # Otherwise, action is the one which has the max Q value\n",
    "        if random.random() < Epsilon:\n",
    "            Q_value = 0\n",
    "            action = np.zeros([Num_action])\n",
    "            action[random.randint(0, Num_action - 1)] = 1\n",
    "        else:\n",
    "            Q_value = output.eval(feed_dict={x_image: [observation_stack], x_sensor: [state_stack]})\n",
    "            action = np.zeros([Num_action])\n",
    "            action[np.argmax(Q_value)] = 1\n",
    "    else:\n",
    "        # Max Q action \n",
    "        Q_value = output.eval(feed_dict={x_image: [observation_stack], x_sensor: [state_stack]})\n",
    "        action = np.zeros([Num_action])\n",
    "        action[np.argmax(Q_value)] = 1\n",
    "        \n",
    "    return action, Q_value\n",
    "\n",
    "def train(Replay_memory, sess, step):\n",
    "    # Select minibatch\n",
    "    minibatch =  random.sample(Replay_memory, Num_batch)\n",
    "\n",
    "    # Save the each batch data\n",
    "    observation_batch      = [batch[0] for batch in minibatch]\n",
    "    state_batch            = [batch[1] for batch in minibatch]\n",
    "    action_batch           = [batch[2] for batch in minibatch]\n",
    "    reward_batch           = [batch[3] for batch in minibatch]\n",
    "    observation_next_batch = [batch[4] for batch in minibatch]\n",
    "    state_next_batch       = [batch[5] for batch in minibatch]\n",
    "    terminal_batch \t       = [batch[6] for batch in minibatch]\n",
    "\n",
    "    # Update target network according to the Num_update value\n",
    "    if step % Num_update == 0:\n",
    "        assign_network_to_target()\n",
    "    \n",
    "    ####################################### Double Q Learning part #######################################\n",
    "    # Get target values\n",
    "    y_batch = []\n",
    "    # Selecting actions\n",
    "    Q_network = output.eval(feed_dict = {x_image: observation_next_batch, x_sensor: state_next_batch})\n",
    "\n",
    "    a_max = []\n",
    "    for i in range(Q_network.shape[0]):\n",
    "        a_max.append(np.argmax(Q_network[i]))\n",
    "\n",
    "    # Evaluation\n",
    "    Q_target = output_target.eval(feed_dict = {x_image: observation_next_batch, x_sensor: state_next_batch})\n",
    "    for i in range(len(minibatch)):\n",
    "        if terminal_batch[i] == True:\n",
    "            y_batch.append(reward_batch[i])\n",
    "        else:\n",
    "            y_batch.append(reward_batch[i] + Gamma * Q_target[i, a_max[i]])\n",
    "\n",
    "    _, loss = sess.run([train_step, Loss], feed_dict = {action_target: action_batch, \n",
    "                                                        y_target: y_batch, \n",
    "                                                        x_image: observation_batch,\n",
    "                                                        x_sensor: state_batch})\n",
    "    ######################################################################################################\n",
    "\n",
    "# Experience Replay \n",
    "def Experience_Replay(progress, Replay_memory, obs_stack, s_stack, action, reward, next_obs_stack, next_s_stack, terminal):\n",
    "    if progress != 'Testing':\n",
    "        # If length of replay memeory is more than the setting value then remove the first one\n",
    "        if len(Replay_memory) > Num_replay_memory:\n",
    "            del Replay_memory[0]\n",
    "\n",
    "        # Save experience to the Replay memory\n",
    "        Replay_memory.append([obs_stack, s_stack, action, reward, next_obs_stack, next_s_stack, terminal])\n",
    "    else:\n",
    "        # Empty the replay memory if testing\n",
    "        Replay_memory = []\n",
    "    \n",
    "    return Replay_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "Replay_memory = []\n",
    "\n",
    "step = 1\n",
    "score = 0\n",
    "score_board = 0\n",
    "\n",
    "episode = 0\n",
    "step_per_episode = 0\n",
    "\n",
    "speed_list = []\n",
    "overtake_list = []\n",
    "lanechange_list = []\n",
    "\n",
    "train_mode = True\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "observation_stack, observation_set, state_stack, state_set = input_initialization(env_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_plot = 0\n",
    "\n",
    "# Training & Testing\n",
    "while True:\n",
    "   \n",
    "    # Get Progress, train mode\n",
    "    progress, train_mode, Epsilon  = get_progress(step, Epsilon)\n",
    "    \n",
    "    # Select Actions \n",
    "    action, Q_value = select_action(progress, sess, observation_stack, state_stack, Epsilon)\n",
    "    action_in = [np.argmax(action)]\n",
    "    \n",
    "    # Get information for plotting\n",
    "    vehicle_speed  = 100 * env_info.vector_observations[0][-8]\n",
    "    num_overtake   = env_info.vector_observations[0][-7]\n",
    "    num_lanechange = env_info.vector_observations[0][-6]\n",
    "    \n",
    "    # Get information for update\n",
    "    env_info = env.step(action_in)[default_brain]\n",
    "\n",
    "    next_observation_stack, observation_set, next_state_stack, state_set = resize_input(env_info, observation_set, state_set) \n",
    "    reward = env_info.rewards[0]\n",
    "    terminal = env_info.local_done[0]\n",
    "    \n",
    "    if progress == 'Training':\n",
    "        # Train!! \n",
    "        train(Replay_memory, sess, step)\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        if step == Num_start_training + Num_training:\n",
    "            save_path = saver.save(sess, '../saved_networks/' + date_time + '_' + algorithm + '_both' + \"/model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    # If progress is finished -> close! \n",
    "    if progress == 'Finished':\n",
    "        print('Finished!!')\n",
    "        env.close()\n",
    "        break\n",
    "        \n",
    "    Replay_memory = Experience_Replay(progress, \n",
    "                                      Replay_memory, \n",
    "                                      observation_stack, \n",
    "                                      state_stack,\n",
    "                                      action, \n",
    "                                      reward, \n",
    "                                      next_observation_stack,\n",
    "                                      next_state_stack,\n",
    "                                      terminal)\n",
    "    \n",
    "    # Update information\n",
    "    step += 1\n",
    "    score += reward\n",
    "    step_per_episode += 1\n",
    "    \n",
    "    observation_stack = next_observation_stack\n",
    "    state_stack = next_state_stack\n",
    "    \n",
    "    # Update tensorboard\n",
    "    if progress != 'Observing':\n",
    "        speed_list.append(vehicle_speed)\n",
    "        \n",
    "        if episode % Num_plot_episode == 0 and check_plot == 1 and episode != 0:\n",
    "            avg_speed      = sum(speed_list) / len(speed_list)\n",
    "            avg_overtake   = sum(overtake_list) / len(overtake_list)\n",
    "            avg_lanechange = sum(lanechange_list) / len(lanechange_list)\n",
    "            \n",
    "            tensorboard_info = [avg_speed, avg_overtake, avg_lanechange]\n",
    "            for i in range(len(tensorboard_info)):\n",
    "                sess.run(update_ops[i], feed_dict = {summary_placeholders[i]: float(tensorboard_info[i])})\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            score_board = 0\n",
    "            \n",
    "            speed_list = []\n",
    "            overtake_list = []\n",
    "            lanechange_list = []\n",
    "\n",
    "            check_plot = 0\n",
    "            \n",
    "    # If terminal is True\n",
    "    if terminal == True:\n",
    "        # Print informations\n",
    "        print('step: ' + str(step) + ' / '  + 'episode: ' + str(episode) + ' / ' + 'progress: ' + progress  + ' / ' + 'epsilon: ' + str(Epsilon)  +' / ' + 'score: ' + str(score))\n",
    "\n",
    "        check_plot = 1\n",
    "\n",
    "        if progress != 'Observing':\n",
    "            episode += 1\n",
    "            \n",
    "            score_board += score\n",
    "            overtake_list.append(num_overtake)\n",
    "            lanechange_list.append(num_lanechange)\n",
    "        \n",
    "            \n",
    "        score = 0\n",
    "        step_per_episode = 0\n",
    "\n",
    "        # Initialize game state\n",
    "        env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "        observation_stack, observation_set, state_stack, state_set = input_initialization(env_info)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
